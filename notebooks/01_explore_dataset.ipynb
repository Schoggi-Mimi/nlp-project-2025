{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "df = pd.read_parquet(\"hf://datasets/argilla/medical-domain/data/train-00000-of-00001-67e4e7207342a623.parquet\")\n",
    "\n",
    "def extract_label(pred):\n",
    "    if isinstance(pred, (list, np.ndarray)) and len(pred) > 0 and isinstance(pred[0], dict):\n",
    "        return pred[0].get(\"label\")\n",
    "    return None\n",
    "\n",
    "df['label'] = df['prediction'].apply(extract_label)\n",
    "df['text_length'] = df['metrics'].apply(lambda x: x.get('text_length') if isinstance(x, dict) else None)\n",
    "\n",
    "# drop empty columns\n",
    "df = df.drop(columns=['inputs', 'prediction', 'prediction_agent', 'annotation', 'annotation_agent', 'multi_label', 'explanation', 'metadata', 'status', 'event_timestamp', 'metrics'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.shape)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text_length",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "bf855a82-a3d9-48b8-92e9-5133123efccc",
       "rows": [
        [
         "0",
         "str",
         "str",
         "str",
         "int"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>str</td>\n",
       "      <td>str</td>\n",
       "      <td>str</td>\n",
       "      <td>int</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  text   id label text_length\n",
       "0  str  str   str         int"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.map(lambda x: type(x).__name__).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Statistics:\n",
      "  total_samples: 4966\n",
      "  missing_values: 0\n",
      "  unique_samples: 2357\n",
      "  class_distribution: {' Surgery': 1088, ' Consult - History and Phy.': 516, ' Cardiovascular / Pulmonary': 371, ' Orthopedic': 355, ' Radiology': 273, ' General Medicine': 259, ' Gastroenterology': 224, ' Neurology': 223, ' SOAP / Chart / Progress Notes': 166, ' Urology': 156, ' Obstetrics / Gynecology': 155, ' Discharge Summary': 108, ' ENT - Otolaryngology': 96, ' Neurosurgery': 94, ' Hematology - Oncology': 90, ' Ophthalmology': 83, ' Nephrology': 81, ' Emergency Room Reports': 75, ' Pediatrics - Neonatal': 70, ' Pain Management': 61, ' Psychiatry / Psychology': 53, ' Office Notes': 50, ' Podiatry': 47, ' Dermatology': 29, ' Cosmetic / Plastic Surgery': 27, ' Dentistry': 27, ' Letters': 23, ' Physical Medicine - Rehab': 21, ' Sleep Medicine': 20, ' Endocrinology': 19, ' Bariatrics': 18, ' IME-QME-Work Comp etc.': 16, ' Chiropractic': 14, ' Rheumatology': 10, ' Diets and Nutritions': 10, ' Speech - Language': 9, ' Lab Medicine - Pathology': 8, ' Autopsy': 8, ' Allergy / Immunology': 7, ' Hospice - Palliative Care': 6}\n",
      "  num_classes: 40\n",
      "  class_balance_ratio: 181.33333333333334\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from src.exploration import DatasetExplorer\n",
    "\n",
    "explorer = DatasetExplorer(df, text_column='text', label_column='label')\n",
    "\n",
    "basic_stats = explorer.compute_basic_stats()\n",
    "print(\"Basic Statistics:\")\n",
    "for key, value in basic_stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1444b757",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "- Some clinical notes might be repeated (e.g., same “standard template” used multiple times).\n",
    "- high imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Length Statistics:\n",
      "\n",
      "Character Length:\n",
      "  min       :      11.00\n",
      "  max       :   18425.00\n",
      "  avg       :    3052.31\n",
      "  median    :    2667.00\n",
      "  std       :    1993.88\n",
      "\n",
      "Token Length:\n",
      "  min       :       3.00\n",
      "  max       :    3489.00\n",
      "  avg       :     553.02\n",
      "  median    :     485.00\n",
      "  std       :     361.98\n"
     ]
    }
   ],
   "source": [
    "# Measure text lengths\n",
    "length_stats = explorer.measure_text_length()\n",
    "print(\"Text Length Statistics:\")\n",
    "print(\"\\nCharacter Length:\")\n",
    "for metric, value in length_stats['char_length'].items():\n",
    "    print(f\"  {metric:<10}: {value:>10.2f}\")\n",
    "print(\"\\nToken Length:\")\n",
    "for metric, value in length_stats['token_length'].items():\n",
    "    print(f\"  {metric:<10}: {value:>10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa96b18",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "- most notes are around 500 words, but some are very long.\n",
    "- std shows very uneven text lenghts. -> truncate or pad to a max sequence token length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Most Frequent Terms:\n",
      "  ,         :     177985\n",
      "  the       :     149782\n",
      "  .         :     140132\n",
      "  and       :      82612\n",
      "  was       :      71766\n",
      "  of        :      59010\n",
      "  to        :      50164\n",
      "  :         :      48649\n",
      "  a         :      42608\n",
      "  with      :      35803\n",
      "  in        :      32758\n",
      "  is        :      26378\n",
      "  patient   :      24108\n",
      "  no        :      17829\n",
      "  she       :      17593\n",
      "  for       :      17047\n",
      "  he        :      15544\n",
      "  were      :      15535\n",
      "  on        :      14654\n",
      "  this      :      13857\n",
      "\n",
      "Top 20 Most Frequent Filtered Terms:\n",
      "  patient   :      24108\n",
      "  right     :      11162\n",
      "  left      :      10853\n",
      "  history   :       9376\n",
      "  procedure :       7179\n",
      "  placed    :       6962\n",
      "  normal    :       6917\n",
      "  well      :       5851\n",
      "  pain      :       5360\n",
      "  also      :       4325\n",
      "  using     :       4121\n",
      "  blood     :       3917\n",
      "  time      :       3896\n",
      "  mg        :       3883\n",
      "  noted     :       3877\n",
      "  performed :       3792\n",
      "  skin      :       3752\n",
      "  without   :       3730\n",
      "  's        :       3668\n",
      "  incision  :       3570\n"
     ]
    }
   ],
   "source": [
    "# Get term frequency\n",
    "term_freq, filtered_term_freq = explorer.compute_term_frequency(top_n=20)\n",
    "print(\"Top 20 Most Frequent Terms:\")\n",
    "for term, count in term_freq:\n",
    "    print(f\"  {term:<10}: {count:>10}\")\n",
    "print(\"\\nTop 20 Most Frequent Filtered Terms:\")\n",
    "for term, count in filtered_term_freq:\n",
    "    print(f\"  {term:<10}: {count:>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b3691b",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "- punctuation and common english stopwords are among the most frequent terms.\n",
    "- \":\" being so common suggests that the notes have many colon separated phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Distribution:\n",
      "  en:  4955 (99.78%)\n",
      "  pt:     3 ( 0.06%)\n",
      "  so:     2 ( 0.04%)\n",
      "  de:     2 ( 0.04%)\n",
      "  tl:     2 ( 0.04%)\n",
      "  ro:     2 ( 0.04%)\n",
      "Non-English Samples:\n",
      "14                                                                   SUBJECTIVE:,\n",
      "34                                  MANNER OF DEATH: , Homicide.,CAUSE OF DEATH:,\n",
      "613                                                       REASON FOR EVALUATION:,\n",
      "1243            XYZ, D.C.,60 Evergreen Place,Suite 902,East Orange, NJ  07018,Re:\n",
      "1381                                                      REASON FOR EVALUATION:,\n",
      "1638                                                                  OPERATION:,\n",
      "1715                                                               INDICATION:  ,\n",
      "2804    REASON FOR CONSULTATION: , Loculated left effusion, multilobar pneumonia.\n",
      "3205                                                               INDICATION:  ,\n",
      "3886                                                                 SUBJECTIVE:,\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Detect languages\n",
    "lang_dist, non_en = explorer.detect_languages()\n",
    "print(\"Language Distribution:\")\n",
    "for lang, stats in lang_dist.items():\n",
    "    print(f\"  {lang}: {stats['count']:>5} ({stats['percentage']:>5}%)\")\n",
    "print(\"Non-English Samples:\")\n",
    "print(non_en.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf497c5",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "- Most notes are in english\n",
    "- The non-english examples are either very short, have a template-like structure or are medical headings. (detectino noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13096a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from src.exploration import TextClusterer\n",
    "\n",
    "texts = df['text'].tolist()\n",
    "labels_true = df['label'].astype('category').cat.codes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d738eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering metrics:\n",
      "silhouette  : 0.0064\n",
      "v_measure   : 0.2252\n",
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0: chief complaint, chief, complaint, history, mg, patient, reveals, normal\n",
      "Cluster 1: artery, coronary, left, right, aortic, coronary artery, stenosis, valve\n",
      "Cluster 2: patient, placed, procedure, right, incision, left, using, anesthesia\n",
      "Cluster 3: patient, right, normal, left, pain, exam, unremarkable, evidence\n",
      "Cluster 4: history, patient, mg, pain, normal, denies, past, daily\n"
     ]
    }
   ],
   "source": [
    "cl = TextClusterer(n_clusters=5)\n",
    "labels, metrics = cl.fit(texts, true_labels=labels_true)\n",
    "\n",
    "print(\"Clustering metrics:\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k:12}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nTop terms per cluster:\")\n",
    "cl.top_terms_per_cluster(n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "523b5eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthias/miniconda3/envs/fromNLPtoLLM/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/matthias/miniconda3/envs/fromNLPtoLLM/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'silhouette': 0.011199906926716552, 'v_measure': 0.23569455086644714}\n",
      "Clustering metrics with stemming:\n",
      "silhouette  : 0.0112\n",
      "v_measure   : 0.2357\n",
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0: histori, patient, wa, ha, mg, medic, pain, daili\n",
      "Cluster 1: wa, left, right, arteri, normal, patient, coronari, procedur\n",
      "Cluster 2: wa, patient, place, procedur, use, incis, oper, right\n",
      "Cluster 3: normal, neg, mass, ear, intact, clear, bilater, reveal\n",
      "Cluster 4: hi, ha, wa, patient, pain, histori, thi, ani\n"
     ]
    }
   ],
   "source": [
    "cl_stem = TextClusterer(n_clusters=5, stemming=True)\n",
    "labels_stem, metrics_stem = cl_stem.fit(texts, true_labels=labels_true)\n",
    "print(metrics_stem)\n",
    "\n",
    "print(\"Clustering metrics with stemming:\")\n",
    "for k, v in metrics_stem.items():\n",
    "    print(f\"{k:12}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nTop terms per cluster:\")\n",
    "cl_stem.top_terms_per_cluster(n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d2302aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthias/miniconda3/envs/fromNLPtoLLM/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'silhouette': 0.005425755779072793, 'v_measure': 0.24227657560716775}\n",
      "Clustering metrics with lemmatization:\n",
      "silhouette  : 0.0054\n",
      "v_measure   : 0.2423\n",
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0: patient, history, mg, pain, day, deny, daily, time\n",
      "Cluster 1: artery, coronary, coronary artery, right, aortic, catheter, valve, stenosis\n",
      "Cluster 2: patient, place, right, procedure, suture, incision, remove, diagnosis\n",
      "Cluster 3: normal, negative, history, patient, clear, bilaterally, pain, masse\n",
      "Cluster 4: patient, place, right, tendon, screw, medial, lateral, bone\n"
     ]
    }
   ],
   "source": [
    "cl_lemma = TextClusterer(n_clusters=5, lemmatization=True)\n",
    "labels_lemma, metrics_lemma = cl_lemma.fit(texts, true_labels=labels_true)\n",
    "print(metrics_lemma)\n",
    "\n",
    "print(\"Clustering metrics with lemmatization:\")\n",
    "for k, v in metrics_lemma.items():\n",
    "    print(f\"{k:12}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nTop terms per cluster:\")\n",
    "cl_lemma.top_terms_per_cluster(n=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fba5ca0",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "- low silhouette -> not well separated clusters in TF-IDF space (expected in medical text where documents overlap in content)\n",
    "- low v-measure -> defines agreement with true class labels (expected as clustering != classification)\n",
    "- clustering works but it groups text by surface similarity, even if overall numeric scores are low\n",
    "- cluster keywords make semantic sense given the medical dataset\n",
    "- stemming slightly increases cluster compactness (higher silhouette) but reduces readability due to word truncation\n",
    "- lemmatization produces cleaner, semantically more consistent clusters (higher v-measure) and is best for interpretability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hello",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
