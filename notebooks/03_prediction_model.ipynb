{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99af13b4",
   "metadata": {},
   "source": [
    "# Task 3: Pre-trained transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33609880-7542-405a-80f2-7fb0c7b4fa54",
   "metadata": {},
   "source": [
    "### Aim\n",
    "In this task, the aim is to train different algorithm to be able to classify correctly our medical transcritped notes. Classifcations are labels directly extracted from argilla dataset, as shown in task 1 (e.g. surgery, orthopedics, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc86b9b",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4993247-0733-470b-9838-f269a9146c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib \n",
    "import transformers \n",
    "import pandas as pd\n",
    "import tqdm \n",
    "import torch \n",
    "import spacy \n",
    "import nltk \n",
    "import langdetect\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10044dda",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. Dataset import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d416578-d1e1-4c29-89f6-6086ee06a77a",
   "metadata": {},
   "source": [
    "We re-use code from task 1 to import our argilla dataset, where we will only keep the text and the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a71210c2-f679-42ac-8a92-ce58bd419d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                                                                                                                                                                                          text  \\\n",
      "0     PREOPERATIVE DIAGNOSIS:,  Iron deficiency anemia.,POSTOPERATIVE DIAGNOSIS:,  Diverticulosis.,PROCEDURE:,  Colonoscopy.,MEDICATIONS: , MAC.,PROCEDURE: , The Olympus pediatric variable colonoscope w...   \n",
      "1     CLINICAL INDICATION:  ,Normal stress test.,PROCEDURES PERFORMED:,1.  Left heart cath.,2.  Selective coronary angiography.,3.  LV gram.,4.  Right femoral arteriogram.,5.  Mynx closure device.,PROCE...   \n",
      "2     FINDINGS:,Axial scans were performed from L1 to S2 and reformatted images were obtained in the sagittal and coronal planes.,Preliminary scout film demonstrates anterior end plate spondylosis at T1...   \n",
      "3     PREOPERATIVE DIAGNOSIS: , Blood loss anemia.,POSTOPERATIVE DIAGNOSES:,1.  Diverticulosis coli.,2.  Internal hemorrhoids.,3.  Poor prep.,PROCEDURE PERFORMED:,  Colonoscopy with photos.,ANESTHESIA: ...   \n",
      "4     REASON FOR VISIT:  ,Elevated PSA with nocturia and occasional daytime frequency.,HISTORY: , A 68-year-old male with a history of frequency and some outlet obstructive issues along with irritative ...   \n",
      "...                                                                                                                                                                                                       ...   \n",
      "4961  SINGLE CHAMBER PACEMAKER IMPLANTATION,PREOPERATIVE DIAGNOSIS: , Mobitz type II block with AV dissociation and syncope.,POSTOPERATIVE DIAGNOSIS: , Mobitz type II block, status post single chamber p...   \n",
      "4962  PROCEDURE:  ,Caudal epidural steroid injection without fluoroscopy.,ANESTHESIA:,  Local sedation.,VITAL SIGNS: , See nurse's records.,PROCEDURE DETAILS: , INT was placed.  The patient was in the p...   \n",
      "4963  PREOPERATIVE DIAGNOSIS: , Hemangioma, nasal tip.,POSTOPERATIVE DIAGNOSIS:,  Hemangioma, nasal tip.,PROCEDURE PERFORMED:  ,1.  Debulking of hemangioma of the nasal tip through an open rhinoplasty a...   \n",
      "4964  PREOPERATIVE DIAGNOSIS: , Right trigger thumb.,POSTOPERATIVE DIAGNOSIS:,  Right trigger thumb.,SURGERY: , Release of A1 pulley, CPT code 26055.,ANESTHESIA:,  General LMA.,TOURNIQUET TIME:  ,9 minu...   \n",
      "4965  PREOPERATIVE DIAGNOSES:,1.  Fullness in right base of the tongue.,2.  Chronic right ear otalgia.,POSTOPERATIVE DIAGNOSIS: , Pending pathology.,PROCEDURE PERFORMED: , Microsuspension direct laryngo...   \n",
      "\n",
      "                                        id                           label  \\\n",
      "0     00001265-03e2-47b2-b6cf-bed32dad2fa9                Gastroenterology   \n",
      "1     0007edf0-1413-4b16-8212-3a13c2ab4e43                         Surgery   \n",
      "2     00097d1e-1357-4447-a39a-fe8f8b7c36ae                       Radiology   \n",
      "3     001622b6-0182-4fee-9881-ae15e81ce836                         Surgery   \n",
      "4     0029245f-8b45-4796-ba09-7760612289c6   SOAP / Chart / Progress Notes   \n",
      "...                                    ...                             ...   \n",
      "4961  ffcee6ac-5e6e-4d2b-97fb-9e6df2e104e2      Cardiovascular / Pulmonary   \n",
      "4962  ffe7b99f-b908-451b-9c41-a2ade089be70                 Pain Management   \n",
      "4963  fffa5f16-1a1d-407c-adaf-d9ae9ac57667                         Surgery   \n",
      "4964  fffb3229-1f56-4502-9a15-8cf97ba53c43                      Orthopedic   \n",
      "4965  ffff14b1-05ef-4224-9aa6-2d8ab3ba6eb4                         Surgery   \n",
      "\n",
      "      text_length  \n",
      "0            1085  \n",
      "1            1798  \n",
      "2            1141  \n",
      "3            1767  \n",
      "4            1519  \n",
      "...           ...  \n",
      "4961         3919  \n",
      "4962          772  \n",
      "4963         3697  \n",
      "4964         2070  \n",
      "4965         2867  \n",
      "\n",
      "[4966 rows x 4 columns]>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "df = pd.read_parquet(\"hf://datasets/argilla/medical-domain/data/train-00000-of-00001-67e4e7207342a623.parquet\")\n",
    "\n",
    "def extract_label(pred):\n",
    "    if isinstance(pred, (list, np.ndarray)) and len(pred) > 0 and isinstance(pred[0], dict):\n",
    "        return pred[0].get(\"label\")\n",
    "    return None\n",
    "\n",
    "df['label'] = df['prediction'].apply(extract_label)\n",
    "df['text_length'] = df['metrics'].apply(lambda x: x.get('text_length') if isinstance(x, dict) else None)\n",
    "\n",
    "# drop empty columns\n",
    "df = df.drop(columns=['inputs', 'prediction', 'prediction_agent', 'annotation', 'annotation_agent', 'multi_label', 'explanation', 'metadata', 'status', 'event_timestamp', 'metrics'], errors='ignore')\n",
    "\n",
    "#print(df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4833a69f-bf98-4728-9b3b-d18643d2e0ad",
   "metadata": {},
   "source": [
    "## 2. Baseline ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c7854-b9dd-44c4-ae32-c888f11dc355",
   "metadata": {},
   "source": [
    "We will try the 3 propopsed algorithms ( linear regression, linear SVM and XGboost) and pick the best performing one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ce073-c691-418e-80bd-b1379d190e17",
   "metadata": {},
   "source": [
    "### 2.1 Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "75a6c79f-575d-4f67-bc63-bc7350dcfa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#0. Split data set into train/test\n",
    "#################################\n",
    "# This code is inspired from : https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X=df[\"text\"]\n",
    "y=df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42) # I split the text : 80% training, 20% test\n",
    "############################\n",
    "# 1. TF-IFD\n",
    "############################\n",
    "\n",
    "# Using sklearn TfidfVectorizer, we can directly pre-processed our text:\n",
    "# - everything in lowercase\n",
    "# - tokenize words\n",
    "# - every feature of same length \n",
    "\n",
    "# We finally return the inverse frequency of each token according to all documents.\n",
    "\n",
    "## This code is adapted from https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(strip_accents=\"unicode\", # I want to strip all accents\n",
    "                             lowercase=True,  # I want everything lowercase\n",
    "                             stop_words=\"english\", # I want to delete common stop words in english\n",
    "                             min_df=5,  # I want words to be at least in 5 documents\n",
    "                             max_df=0.8 # very frequent words are not useful to distinguish between documents\n",
    "                            ) \n",
    "\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test=vectorizer.transform(X_test) # I transform X_test according to X_train frequency per document over apperance in every documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d3c4e9-919a-45cd-9e04-ab28776a1b75",
   "metadata": {},
   "source": [
    "### 2.2 Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bff0ecaf-36e0-4213-8530-d45c13e2df04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score macro SVM:  0.1643204293076342\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "SVM=LinearSVC(random_state=0, tol=1e-5,class_weight=\"balanced\")\n",
    "SVM.fit(X_train,y_train)\n",
    "\n",
    "SVM.score(X_test,y_test) # Accuracy\n",
    "\n",
    "f1_score_macro_SVM=f1_score(y_test, SVM.predict(X_test), average='macro') # Macro F_1 score -->\"harmonic mean of the precision and recall\" https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "print(\"F1 score macro SVM: \",f1_score_macro_SVM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16873f9-9c4b-40de-a282-da9daf8f5f30",
   "metadata": {},
   "source": [
    "## 2.3 Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "350d9c74-8198-4e70-8859-9df4a8e9a587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score macro LR:  0.3944886061291781\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR=LogisticRegression(random_state=0, tol=1e-5,class_weight=\"balanced\") # we have 40 categories, but some are over-represented. Therefore, we balanced\n",
    "                                                                 # weights according to their initial frequency in training set\n",
    "LR.fit(X_train,y_train)\n",
    "\n",
    "LR.score(X_test,y_test)\n",
    "\n",
    "f1_score_macro_LR=f1_score(y_test, LR.predict(X_test), average='macro')\n",
    "print(\"F1 score macro LR: \",f1_score_macro_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482fd7dd-5358-44f9-a05b-763c721ee51d",
   "metadata": {},
   "source": [
    "### 2.4 XGboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020eb1e7-4637-488c-96c7-03a89d193748",
   "metadata": {},
   "source": [
    "Considering the high dimensionality of our data , XGboost takes too much time to run and SVM or LR are already strong baseline ML algorithm to compare our transformers too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b4d39-66de-4fc3-8763-a2f1bae2adbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
